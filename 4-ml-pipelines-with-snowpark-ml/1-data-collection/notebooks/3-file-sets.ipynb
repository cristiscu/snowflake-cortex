{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SnowflakeLoginOptions() is in private preview since 0.2.0. Do not use it in production. \n"
     ]
    }
   ],
   "source": [
    "from snowflake.snowpark import Session\n",
    "from snowflake.ml.utils.connection_params import SnowflakeLoginOptions\n",
    "\n",
    "session = Session.builder.configs(SnowflakeLoginOptions(\"test_conn\")).create()\n",
    "STAGE = \"@TEST.PUBLIC.ENC_STAGE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a FileSet from a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:snowflake.snowpark:FileSet.files() is in private preview since 0.2.0. Do not use it in production. \n",
      "WARNING:snowflake.snowpark:SFFileSystem.ls() is in private preview since 0.2.0. Do not use it in production. \n",
      "WARNING:snowflake.snowpark:SFStageFileSystem.ls() is in private preview since 0.2.0. Do not use it in production. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sfc://@TEST.PUBLIC.ENC_STAGE/housing1/data_01b3efd4-0002-5aa8-005b-3f070006b7e2_013_3_0.snappy.parquet']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snowflake.ml.fileset import fileset\n",
    "\n",
    "session.sql(f\"REMOVE {STAGE}/housing1\").collect()\n",
    "fs1 = fileset.FileSet.make(\n",
    "    target_stage_loc=STAGE,\n",
    "    name=\"housing1\",\n",
    "    snowpark_dataframe=session.table('TEST.PUBLIC.HOUSING'),\n",
    "    shuffle=True)\n",
    "fs1.files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a FileSet from a query result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sfc://@TEST.PUBLIC.ENC_STAGE/housing2/data_01b3efd4-0002-5b19-005b-3f070006887a_013_4_0.snappy.parquet']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql(f\"REMOVE {STAGE}/housing2\").collect()\n",
    "fs2 = fileset.FileSet.make(\n",
    "    target_stage_loc=STAGE,\n",
    "    name=\"housing2\",\n",
    "    sf_connection=session.connection,\n",
    "    query=\"select * from TEST.PUBLIC.HOUSING\",\n",
    "    shuffle=True)\n",
    "fs2.files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed first FileSet to TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:snowflake.snowpark:FileSet.to_tf_dataset() is in private preview since 0.2.0. Do not use it in production. \n",
      "WARNING:snowflake.snowpark:SFFileSystem.optimize_read() is in private preview since 0.2.0. Do not use it in production. \n",
      "WARNING:snowflake.snowpark:SFStageFileSystem.optimize_read() is in private preview since 0.2.0. Do not use it in production. \n",
      "WARNING:snowflake.snowpark:SFFileSystem.info() is in private preview since 0.2.0. Do not use it in production. \n",
      "WARNING:snowflake.snowpark:SFFileSystem._open() is in private preview since 0.2.0. Do not use it in production. \n",
      "WARNING:snowflake.snowpark:SFStageFileSystem._open() is in private preview since 0.2.0. Do not use it in production. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LONGITUDE': <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-121.96, -118.29, -122.2 , -118.27], dtype=float32)>, 'LATITUDE': <tf.Tensor: shape=(4,), dtype=float32, numpy=array([37.27, 33.91, 37.76, 34.25], dtype=float32)>, 'HOUSING_MEDIAN_AGE': <tf.Tensor: shape=(4,), dtype=float32, numpy=array([31., 41., 37., 37.], dtype=float32)>, 'TOTAL_ROOMS': <tf.Tensor: shape=(4,), dtype=float32, numpy=array([3347., 2475., 2680., 2489.], dtype=float32)>, 'TOTAL_BEDROOMS': <tf.Tensor: shape=(4,), dtype=float32, numpy=array([589., 532., 736., 454.], dtype=float32)>, 'POPULATION': <tf.Tensor: shape=(4,), dtype=float32, numpy=array([1566., 1416., 1925., 1215.], dtype=float32)>, 'HOUSEHOLDS': <tf.Tensor: shape=(4,), dtype=float32, numpy=array([597., 470., 667., 431.], dtype=float32)>, 'MEDIAN_INCOME': <tf.Tensor: shape=(4,), dtype=float32, numpy=array([5.5151, 3.8372, 1.4097, 5.0234], dtype=float32)>, 'MEDIAN_HOUSE_VALUE': <tf.Tensor: shape=(4,), dtype=float32, numpy=array([286800., 156400.,  84600., 257600.], dtype=float32)>, 'OCEAN_PROXIMITY': <tf.Tensor: shape=(4,), dtype=string, numpy=\n",
      "array([b'<1H OCEAN', b'<1H OCEAN', b'NEAR BAY', b'<1H OCEAN'],\n",
      "      dtype=object)>}\n"
     ]
    }
   ],
   "source": [
    "ds1 = fs1.to_tf_dataset(batch_size=4, shuffle=True, drop_last_batch=True)\n",
    "for batch in ds1:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed second FileSet to PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:snowflake.snowpark:FileSet.to_torch_datapipe() is in private preview since 0.2.0. Do not use it in production. \n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "(0000) Unable to import torchdata.datapipes.iter.IterableWrapper.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Projects\\snowflake-cortex\\venv\\lib\\site-packages\\snowflake\\ml\\_internal\\telemetry.py:367\u001b[0m, in \u001b[0;36msend_api_usage_telemetry.<locals>.decorator.<locals>.wrap\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 367\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Projects\\snowflake-cortex\\venv\\lib\\site-packages\\snowflake\\snowpark\\_internal\\utils.py:654\u001b[0m, in \u001b[0;36mfunc_decorator.<locals>.wrapper.<locals>.func_call_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    653\u001b[0m warning(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m, warning_text)\n\u001b[1;32m--> 654\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Projects\\snowflake-cortex\\venv\\lib\\site-packages\\snowflake\\ml\\fileset\\fileset.py:42\u001b[0m, in \u001b[0;36m_raise_if_deleted.<locals>.raise_if_deleted_helper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m snowml_exceptions\u001b[38;5;241m.\u001b[39mSnowflakeMLException(\n\u001b[0;32m     39\u001b[0m         error_code\u001b[38;5;241m=\u001b[39merror_codes\u001b[38;5;241m.\u001b[39mSNOWML_DELETE_FAILED,\n\u001b[0;32m     40\u001b[0m         original_exception\u001b[38;5;241m=\u001b[39mfileset_errors\u001b[38;5;241m.\u001b[39mFileSetAlreadyDeletedError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe FileSet has already been deleted.\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     41\u001b[0m     )\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Projects\\snowflake-cortex\\venv\\lib\\site-packages\\snowflake\\ml\\fileset\\fileset.py:370\u001b[0m, in \u001b[0;36mFileSet.to_torch_datapipe\u001b[1;34m(self, batch_size, shuffle, drop_last_batch)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fs\u001b[38;5;241m.\u001b[39moptimize_read(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_list_files())\n\u001b[1;32m--> 370\u001b[0m input_dp \u001b[38;5;241m=\u001b[39m \u001b[43mIterableWrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_list_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch_datapipe_module\u001b[38;5;241m.\u001b[39mReadAndParseParquet(input_dp, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fs, batch_size, shuffle, drop_last_batch)\n",
      "File \u001b[1;32mc:\\Projects\\snowflake-cortex\\venv\\lib\\site-packages\\snowflake\\ml\\_internal\\utils\\import_utils.py:19\u001b[0m, in \u001b[0;36mMissingOptionalDependency.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dep_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: Unable to import torchdata.datapipes.iter.IterableWrapper.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m----> 3\u001b[0m ds2 \u001b[38;5;241m=\u001b[39m \u001b[43mfs2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_torch_datapipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(ds2, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m loader:\n",
      "File \u001b[1;32mc:\\Projects\\snowflake-cortex\\venv\\lib\\site-packages\\snowflake\\ml\\_internal\\telemetry.py:389\u001b[0m, in \u001b[0;36msend_api_usage_telemetry.<locals>.decorator.<locals>.wrap\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m me\u001b[38;5;241m.\u001b[39moriginal_exception \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 389\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m me\u001b[38;5;241m.\u001b[39moriginal_exception \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m update_stmt_params_if_snowpark_df(res, statement_params)\n",
      "\u001b[1;31mImportError\u001b[0m: (0000) Unable to import torchdata.datapipes.iter.IterableWrapper."
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "ds2 = fs2.to_torch_datapipe(batch_size=4, shuffle=True, drop_last_batch=True)\n",
    "loader = DataLoader(ds2, batch_size=None, num_workers=0)\n",
    "for batch in loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
